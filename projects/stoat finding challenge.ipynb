{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoat-finding particle filter\n",
    "\n",
    "Our friend Data Stoat has gone missing! The GPS sensor that they normally carry has stopped working. \n",
    "But Data still has a low-res camera with mobile uplink, so we know what sort of scenery they're in. Can you help find Data Stoat?\n",
    "\n",
    "Submit your answer on Moodle, in the form of a greyscale png file the same dimensions as the map. Your png should be a heatmap, where each pixel is coloured \n",
    "according to the probability that Data Stoat is in that location, white = highest probability, black = zero probability. I will normalize the pixels\n",
    "to form a proper probability distribution, and your score will be the probability you assign to Data Stoat's true location.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://www.cl.cam.ac.uk/teaching/2122/DataSci/data/voronoi-map-goal-16000-shaded.png\" style=\"width:12em; height:12em\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Markov models\n",
    "\n",
    "\n",
    "The underlying probability model is a hidden Markov model,\n",
    "where $X_n$ is the location at timestep $n$ and $Y_n$ is the noisy observation.\n",
    "We wish to compute the distribution of $X_n$ given observations $(y_0,\\dots,y_n)$.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "&X_0& \\to &X_1& \\to &X_2& \\to \\cdots\\\\\n",
    "&\\downarrow&  &\\downarrow& &\\downarrow&\\\\\n",
    "&Y_0& &Y_1& &Y_2&\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We can find this distribution iteratively. Suppose we know the distribution of $(X_{n-1} | y_0,\\dots,y_{n-1})$.\n",
    "Then we can find the distribution of $(X_n | y_0,\\dots,y_{n-1})$ using the law of total probability and memorylessness,\n",
    "\n",
    "$$\n",
    "\\Pr(x_n|y_0,\\dots,y_{n-1}) = \\sum_{x_{n-1}} \\Pr(x_n |x_{n-1}) \\Pr(x_{n-1}|y_0,\\dots,y_{n-1})\n",
    "$$\n",
    "\n",
    "and then we can find the distribution of $(X_n | y_0,\\dots,y_n)$ using Bayes's rule and memorylessness,\n",
    "\n",
    "$$\n",
    "\\Pr(x_n|y_0,\\dots,y_{n-1},y_n) = \\text{const}\\times\\Pr(x_n|y_0,\\dots,y_{n-1}) \\Pr(y_n|x_n).\n",
    "$$\n",
    "\n",
    "The method is laid out in Example Sheet 4, which you should attempt first, before coding. The example sheet asks you to solve the equations exactly, producing a vector of probabilities, $\\Pr(x_n|y_0,\\dots,y_n)$ for each location $x_n$ on the map. But when the map is large, then it's not practical to compute the sum nor the constant. Instead, we can use an empirical approximation.\n",
    "\n",
    "#### Empirical approximation\n",
    "\n",
    "The idea of empirical approximation is that, instead of working with probability distributions, we work with *weighted samples*, and we choose the weights so as approximate the distribution we're interested in. Formally, if we want to approximate the distribution of a random variable $Z$, and we have a collection of points $z_1,\\dots,z_n$, we want weights so that\n",
    "$$\n",
    "\\mathbb{P}(Z\\in A) \\approx \\sum_{i=1}^n w_i 1_{z_i\\in A}.\n",
    "$$\n",
    "We've used weighted samples in two ways in the course:\n",
    "\n",
    "* For the simple Monte Carlo approximation, we sample $z_i$ from $Z$, and we let all the weights be the same, $1/n$.\n",
    "\n",
    "* For computational Bayes's rule, we sample $z_i$ from the prior distribution, and let the weights be proportional to the likelihood of the data conditional on $z_i$.\n",
    "\n",
    "This practical will use an empirical approximation technique called a *particle filter*. \n",
    "In the particle filter, we'll use weighted samples to approximate the distributions of $(X_n | y_0,\\dots,y_{n-1})$ and of $(X_n|y_0,\\dots,y_n)$. Each sample is called a *particle*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "import imageio\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "We have data from several animals which are wandering over a terrain. The animals are equipped with GPS \n",
    "and with cameras, but the GPS on animal 0 has stopped working. We would like to find out where this animal is.\n",
    "\n",
    "Here is the terrain and the GPS+camera data. The camera records roughly the rgb values of the animal's current location, though there is some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_image = imageio.imread('https://www.cl.cam.ac.uk/teaching/2223/DataSci/data/voronoi-map-goal-16000-shaded.png')\n",
    "localization = pandas.read_csv('https://www.cl.cam.ac.uk/teaching/2223/DataSci/data/localization_2022.csv')\n",
    "localization.sort_values(['id','t'], inplace=True)\n",
    "\n",
    "# Pull out observations for the animal we want to track\n",
    "observations = localization.loc[localization.id==0, ['r','g','b']].values\n",
    "\n",
    "unique_id = np.unique(localization.id)\n",
    "unique_id = np.delete(unique_id, 0)\n",
    "print(unique_id)\n",
    "# Pull out observations for all other animals\n",
    "observations_test = np.array([localization.loc[localization.id==i, ['r','g','b']].values for i in unique_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = localization\n",
    "\n",
    "fig,(ax,ax2) = plt.subplots(2,1, figsize=(4,5), gridspec_kw={'height_ratios':[4,.5]})\n",
    "ax.imshow(map_image.transpose(1,0,2), alpha=.5)\n",
    "w,h = map_image.shape[:2]\n",
    "ax.set_xlim([0,w])\n",
    "ax.set_ylim([0,h])\n",
    "\n",
    "for i in range(1,5):\n",
    "    ax.plot(df.loc[df.id==i,'x'].values, df.loc[df.id==i,'y'].values, lw=1, label=i)\n",
    "ax.axis('off')\n",
    "ax.legend()\n",
    "ax.set_title('Animals 1--4, GPS tracks')\n",
    "\n",
    "ax2.bar(np.arange(len(observations)), np.ones(len(observations)), color=observations, width=2)\n",
    "ax2.set_xlim([0,len(observations)])\n",
    "ax2.set_yticks([])\n",
    "ax2.set_title('Animal id=0, camera only')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding note.** Given an image `map_image`, we can treat it as a simple array, and get at the pixels with `map_image[i,j]`. I'll interpret this as x-coordinate i and y-coordinate j. However, the plotting routine `ax.imshow` interprets the first coordinate as y and the second as x, so I need to transpose the array before plotting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Empirical approximation of $X_0$\n",
    "\n",
    "We'll start with a prior belief that the animal's location $X_0$ is uniformly distributed over the map.\n",
    "\n",
    "The first step is to create a sample from this initial distribution. We'll store it as an $M\\times 3$ array, one row per particle, with columns for $x$ coordinate, $y$ coordinate, and weight $w$. The prior distribution is uniform, so $w=1/M$. We'll call this matrix `δ0`.\n",
    "\n",
    "Here's also a handy function to visualize the particles, use `show_particles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,H = map_image.shape[:2]\n",
    "M = num_particles = 2000\n",
    "\n",
    "# Empirical representation of the distribution of X0\n",
    "δ0 = np.column_stack([np.random.uniform(0,W-1,size=M), np.random.uniform(0,H-1,size=M), np.ones(M)/M])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_particles(particles, ax=None, s=1, c='red', alpha=.5):\n",
    "    # Plot an array of particles, with size proportional to weight.\n",
    "    # (Scale up the sizes by setting s larger.)\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots(figsize=(2.5,2.5))\n",
    "    ax.imshow(map_image.transpose(1,0,2), alpha=alpha, origin='lower')\n",
    "    w,h = map_image.shape[:2]\n",
    "    ax.set_xlim([0,w])\n",
    "    ax.set_ylim([0,h])\n",
    "    w = particles[:,2]\n",
    "    ax.scatter(particles[:,0],particles[:,1], s=w/np.sum(w)*s, color=c)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(4,4))\n",
    "show_particles(δ0, s=400, ax=ax)\n",
    "ax.set_title('$X_0$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Empirical approximation of $(X_0|y_0)$: updated weights\n",
    "\n",
    "Let's first find the distribution of $(X_0|y_0)$, where $y_0$ is the first observation. Bayes's rule tells us the distribution:\n",
    "\n",
    "$$\n",
    "\\operatorname{Pr}_{X_0}(x|y_0)=\\text{const}\\times \\operatorname{Pr}_{X_0}(x)\\Pr(y_0|X_0=x).\n",
    "$$\n",
    "\n",
    "We've seen the empirical approximation for this, in lecture notes on computational Bayes's rule. We take a sample from $X_0$, and we set the weight of sample point $p$ to be proportional to $\\Pr(y_0|X_0=p)$. We already build ourselves a sample from $X_0$, in the previous section. We'll call our new weighted particles `π0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = observations[0]\n",
    "print(f\"First observation: rgb = {y0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply Bayes's rule, we need a probability model for $Y_0$ given a particle's location. A reasonable guess is that $Y_0$ is a noisy version of the colour patch around the supposed location. Here's a handy utility to extract the average colour of a patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch(im, xy, size=3):\n",
    "    s = (size-1) / 2\n",
    "    nx,ny = np.meshgrid(np.arange(-s,s+1), np.arange(-s,s+1))\n",
    "    nx,ny = np.stack([nx,ny], axis=0).reshape((2,-1))\n",
    "    neighbourhood = np.row_stack([nx,ny])\n",
    "    w,h = im.shape[:2]\n",
    "    neighbours = neighbourhood + np.array(xy).reshape(-1,1)\n",
    "    neighbours = nx,ny = np.round(neighbours).astype(int)\n",
    "    nx,ny = neighbours[:, (nx>=0) & (nx<w) & (ny>=0) & (ny<h)]\n",
    "    patch = im[nx,ny,:3]\n",
    "    return np.mean(patch, axis=0)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = δ0[0,:2]\n",
    "print(f\"First particle is at {loc}\")\n",
    "\n",
    "col = patch(map_image, loc, size=3)\n",
    "print(f\"Map terrain around this particle: rgb = {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #TODO: TASK 1\n",
    "\n",
    "Implement a `pr(y,loc)` function to compute the probability of observing `y` if the true location is `loc`.\n",
    "\n",
    "It's up to you to invent the probability model. A reasonable probability model is that the observed rgb values in `y` are independent Gaussian random variables,\n",
    "with mean `patch(map_image, loc)`, and with a standard deviation that you should pick.\n",
    "    If you want to be cleverer, consider a probability model based on closeness in HSL colour space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "from skimage import color\n",
    "df1 = localization\n",
    "obs = df1.loc[df1.id!=0, ['r','g','b']].values\n",
    "mtest = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "xys = np.column_stack([df.loc[df.id!=0,'x'].values, df.loc[df.id!=0,'y'].values])\n",
    "patches = []\n",
    "for xy in xys:\n",
    "    patches+=[patch(map_image, xy)]\n",
    "\n",
    "patches=np.array(patches)\n",
    "\n",
    "def f(x):\n",
    "    return np.sum(scipy.stats.norm.logpdf(color.deltaE_ciede2000(color.rgb2lab(obs), color.rgb2lab(patches)), 0, scale=x))\n",
    "    \n",
    "xHat = scipy.optimize.fmin(lambda x: -f(np.exp(x)), 0)\n",
    "\n",
    "print(np.exp(xHat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(y, loc):\n",
    "    from skimage import color\n",
    "    rgb0 = patch(map_image, loc)\n",
    "    lab0 = color.rgb2lab(rgb0)\n",
    "    lab1 = color.rgb2lab(y)\n",
    "    dist = color.deltaE_ciede2000(lab0, lab1)\n",
    "    return scipy.stats.norm.pdf(dist, loc=0, scale=9.35992708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "y0 = observations[0]\n",
    "loc = δ0[0,:2]\n",
    "w = pr(y0, loc)\n",
    "import numbers\n",
    "assert isinstance(w, numbers.Number) and w>=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set the weight of all the particles, and thereby obtain a sample of $(X_0|y_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = observations[0]\n",
    "y_test = [obs[0] for obs in observations_test]\n",
    "w = np.array([pr(y0, (x,y)) for x,y,_ in δ0])\n",
    "w_test = np.array([[pr(y_t, (x,y)) for x,y,_ in δ0] for y_t in y_test])\n",
    "π0 = np.copy(δ0)\n",
    "π0[:,2] = w / np.sum(w)\n",
    "π_test = [np.copy(δ0) for _ in range(len(w_test))]\n",
    "for j in range(len(π_test)):\n",
    "    π_test[j][:,2] = w_test[j] / np.sum(w_test[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(axδ,axπ) = plt.subplots(1,2, figsize=(8,4), sharex=True, sharey=True)\n",
    "show_particles(δ0, ax=axδ, s=600)\n",
    "show_particles(π0, ax=axπ, s=600)\n",
    "axπ.add_patch(matplotlib.patches.Rectangle((0,0),100,100,color=y0))\n",
    "axπ.text(50,50,'$y_0$', c='white', ha='center', va='center', fontsize=14)\n",
    "axδ.set_title('$X_0$')\n",
    "axπ.set_title('$(X_0|y_0)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Empirical approximation of $(X_1|y_0)$: wandering particles\n",
    "\n",
    "The next step is to find the distribution of the animal's location, after a timestep. \n",
    "Formally, we want to find the distribution of $(X_1|y_0)$.\n",
    "\n",
    "We believe that the animal's location follows a Markov chain, in other words, that $X_1$ is generated from $X_0$\n",
    "according to some random transition. It's easy to generate a sample of $X_1$ values: just take a sample of $X_0$ values, and apply a random transition to each of of the particles.\n",
    "Likewise, if we have a *weighted* sample representing $(X_0|y_0)$, and we apply a random transition to each particle, and leave the weights unchanged, we'll get a weighted sample representing $(X_1|y_0)$.\n",
    "\n",
    "We'll now compute a weighted sample for $(X_1|y_0)$, and call it `δ1`. To do this, we need a probability model for how the animal moves in each timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #TODO: TASK 2\n",
    "\n",
    "Implement a `walk(loc)` function to simulate the animal's movement in one timestep.\n",
    "\n",
    "A reasonable probability model is that the animal chooses a direction uniformly in the range $[0,2\\pi)$, \n",
    "and then chooses a random distance, for example an Exponential random variable with mean 5.\n",
    "And then truncate the position to ensure it lies on the map &mdash; otherwise the `patch` function won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk(loc):\n",
    "    direction = np.random.uniform(0, 2*np.pi)\n",
    "    radius = np.random.exponential(5)\n",
    "    delta_x = radius * np.cos(direction)\n",
    "    delta_y = radius*np.sin(direction)\n",
    "    new_x = min(max(loc[0] + delta_x, 0), W-1)\n",
    "    new_y = min(max(loc[1] + delta_y, 0), H-1)\n",
    "    return np.array([new_x, new_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "loc = π0[0,:2]\n",
    "loc2 = walk(loc)\n",
    "assert len(loc2)==2 and isinstance(loc2[0], numbers.Number) and isinstance(loc2[1], numbers.Number)\n",
    "assert loc2[0]>=0 and loc2[0]<=W-1 and loc2[1]>=0 and loc2[1]<=H-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply this movement to all the particles, and thereby obtain a sample of $(X_1|y_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "δ1 = np.copy(π0)\n",
    "for i in range(len(δ1)):\n",
    "    δ1[i,:2] = walk(δ1[i,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(4,4))\n",
    "show_particles(π0, ax=ax, s=4000, c='blue', alpha=.25)\n",
    "show_particles(δ1, ax=ax, s=4000, c='red', alpha=.25)\n",
    "ax.set_xlim([200,400])\n",
    "ax.set_ylim([100,300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Iterate\n",
    "\n",
    "Now we can apply these two update steps iteratively, updating the sample based on successive observations.\n",
    "The two steps of the iteration are\n",
    "\n",
    "1. Given a weighted sample $(X_{n-1}|y_0,\\dots,y_{n-1})$,\n",
    "get a weighted sample of $(X_n|y_0,\\dots,y_{n-1})$ by making each particle take a random step.\n",
    "\n",
    "\n",
    "2. Given a weighted sample $(X_n|y_0,\\dots,y_{n-1})$, get a weighted sample of\n",
    "$(X_n|y_0,\\dots,y_n)$ by **multiplying** the weight of each particle $p$ by $\\Pr(y_n|X_n=p)$ and then rescaling weights.\n",
    "\n",
    "\n",
    "(Step 2 is a generalization of the Bayes update rule. The simple Bayes update rule is for when we start with a uniformly-weighted sample from the prior, and this version applies when we start with a weighted sample from the prior.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles = np.copy(π0)\n",
    "for n,obs in enumerate(observations[:50]):\n",
    "    # Compute δ, the locations after a movement step\n",
    "    for i in range(num_particles):\n",
    "        particles[i,:2] = walk(particles[i,:2])\n",
    "    # Compute π, the posterior after observing y\n",
    "    w = particles[:,2]\n",
    "    w *= np.array([pr(obs, (px,py)) for px,py,_ in particles])\n",
    "    particles[:,2] = w / np.sum(w)\n",
    "    # Plot the current particles\n",
    "    fig,ax = plt.subplots(figsize=(3.5,3.5))\n",
    "    show_particles(particles, ax, s=20)\n",
    "    ax.set_title(f\"Timestep {n+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run this, you will likely find that the output is completely useless! The problem is numerical\n",
    "instability. We're only using 2000 samples, and many of these samples get assigned a weight that is almost zero, \n",
    "so we end up with a tiny sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #TODO: TASK 3\n",
    "\n",
    "Plot a histogram of particle weights, after 0, 1, 5, 50, and 100 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(5,1, figsize=(8,6), sharey=True)\n",
    "for n,ax in zip([0,1,5,50,100],axes):\n",
    "    # TODO: let w be the appropriate weights\n",
    "    ax.hist(w, bins=60)\n",
    "    ax.axvline(x=1/len(particles), color='black', linewidth=4, linestyle='dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Particle hygiene\n",
    "\n",
    "Empirical approximations work better when the weights are reasonably well distributed. There's nothing stopping us from modifying our set of particles to balance out the weights, as long as we don't mess up the empirical approximation. For example, we could take a particle with excessively high weight, and split it into two particles each with half the weight. After the next `walk` step, those two particles will diverge, and so we'll hopefully get a good spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #TODO: TASK 4\n",
    "\n",
    "Implement a function `prune_spawn(particles)`. This should delete the lowest-weighted 20% of the particles.\n",
    "Then it should take the highest-weighted 20% of the particles, and split them in two. In other words \n",
    "it should create a duplicate at the same location, and give both the original and the duplicate half the weight. The two versions will diverge in the future, as they take different steps.\n",
    "\n",
    "Apply this function every iteration, and show an animation of the result.\n",
    "\n",
    "After processing 100 observations, you should see something like the picture in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_spawn(particles):\n",
    "    t2 = len(particles)*2//10\n",
    "    p = np.copy(particles)\n",
    "    particles = particles[p[:,-1].argsort()[::-1]]\n",
    "    particles = particles[:-t2]\n",
    "    spawn = particles[:t2]\n",
    "    spawn[:, 2]/=2\n",
    "    spawn = np.repeat(spawn, 2, axis=0)\n",
    "    particles = np.concatenate([spawn, particles[t2:]])\n",
    "    return particles\n",
    "prune_spawn(particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(3.5,3.5))\n",
    "show_particles(particles, ax, s=20)\n",
    "ax.set_title(f\"Timestep {n+1}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0, 0, 0], [1, 1, 1]]\n",
    "x = np.repeat(x, 3, axis=0)\n",
    "print(x)\n",
    "\n",
    "print(len(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles = np.copy(π0)\n",
    "\n",
    "for n,obs in enumerate(observations[:100]):\n",
    "    # Compute δ, the locations after a movement step\n",
    "    for i in range(len(particles)):\n",
    "        particles[i,:2] = walk(particles[i,:2])\n",
    "    # Compute π, the posterior after observing y\n",
    "    w = particles[:,2]\n",
    "    w *= np.array([pr(obs, (px,py)) for px,py,_ in particles])\n",
    "    particles[:,2] = w / np.sum(w)\n",
    "    # Prune/spawn\n",
    "    if (n < 99):\n",
    "        particles = prune_spawn(particles)\n",
    "    if (n+1)%10 ==0:\n",
    "        print(n+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_test = np.copy(π_test)\n",
    "for (i, animal) in enumerate(observations_test):\n",
    "    p = particles_test[i]\n",
    "    print(\"Starting animal: \" + str(i))\n",
    "    for n,obs in enumerate(animal[:100]):\n",
    "        # Compute δ, the locations after a movement step\n",
    "        for i in range(len(p)):\n",
    "            p[i,:2] = walk(p[i,:2])\n",
    "        # Compute π, the posterior after observing y\n",
    "        w = p[:,2]\n",
    "        w *= np.array([pr(obs, (px,py)) for px,py,_ in p])\n",
    "        p[:,2] = w / np.sum(w)\n",
    "        # Prune/spawn\n",
    "        if (n < 99):\n",
    "            p = prune_spawn(p)\n",
    "        if (n+1)%10 ==0:\n",
    "            print(n+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #TODO: TASK 5\n",
    "\n",
    "Smooth your predictions in whatever way you think is appropriate. Create a\n",
    "greyscale png file the same dimensions as the map. Your png should be a heatmap, where each pixel is coloured \n",
    "according to the probability distribution of the final hidden state $X_N$, white = highest probability, black = zero probability.\n",
    "\n",
    "(For scoring, the pixel values will be normalized to sum to one, so it doesn't matter what value you assign to the color white.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = np.array([localization.loc[localization.id==i, ['x','y']].values for i in unique_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglik(theta):\n",
    "    mu, nu, tau = theta\n",
    "    to_return = 0;\n",
    "    y_vec = np.array([location[99] for location in locations])\n",
    "    for (i, y) in enumerate(y_vec):\n",
    "        p = particles_test[i]\n",
    "        means = p[:, :2]\n",
    "        weights = p[:, 2:]\n",
    "        for (j, m) in enumerate(means):\n",
    "            w = weights[j]\n",
    "            to_return += w*scipy.stats.multivariate_normal.logpdf(y, mean=m, cov=[[mu, tau], [tau, nu]])\n",
    "    return to_return\n",
    "\n",
    "def transform(theta):\n",
    "    mu, nu, tau = theta\n",
    "    return [np.exp(mu), np.exp(nu), tau]\n",
    "\n",
    "m_, n_, t_ = scipy.optimize.fmin(lambda theta: -loglik(transform(theta)), [0, 0, 0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.exp(m_))\n",
    "print(np.exp(n_))\n",
    "print(t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(c):\n",
    "    s = 0\n",
    "    x, y = c\n",
    "    means = p[:, :2]\n",
    "    weights = p[:, 2:]\n",
    "    for (j, m) in enumerate(means):\n",
    "        w = weights[j]\n",
    "        s = w*scipy.stats.multivariate_normal.logpdf([x, y], mean=m, cov=[[26283.918003097522, -21.811767226422184], [-21.811767226422184, 43084.63108644604]])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Treat each particle as the mean of a multivariate normal distribution\n",
    "## 500*500*2000 computations here.\n",
    "## TODO: Find a way to speed it up\n",
    "p = [[loglikelihood(tuple((i, j))) for j in range(H)] for i in range(W)]\n",
    "plt.imsave(\"stoat_prediction.png\", p, cmap='gray', vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = scipy.stats.gaussian_kde(np.transpose(particles[:, :2]), weights=particles[:, 2])\n",
    "p = [[kde(tuple((i, j))) for j in range(H)] for i in range(W)]\n",
    "plt.imsave(\"stoat_prediction.png\", p, cmap='gray', vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prediction image looks right\n",
    "pred_image = imageio.imread('stoat_prediction.png')\n",
    "fig,ax = plt.subplots(figsize=(4,4))\n",
    "ax.imshow(map_image.transpose(1,0,2), alpha=.5, origin='lower')\n",
    "ax.imshow(pred_image.transpose(1,0,2), alpha=.5, origin='lower')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
